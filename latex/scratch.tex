% Introduction

% Our first contribution is an algorithm for the joint inference of the shape and pose of an object from its partial point cloud. We jointly optimize the shape, scale, position and orientation of a canonical object to match the observed point cloud. We show our method is $\mathrm{SE}(3)$-invariant in the limit of random restarts. We also propose a simple method for turning the inferred point cloud into meshes. Our second, and more important, contribution is the warping of robot actions. We focus on actions where two objects come into contact: e.g., grasping, relative object placement (e.g. mug on tree) and trajectory cloning with contacts (e.g. painting with a brush on a canvas). We identify the \textit{interaction points} between a pair of objects. For grasps, those are the pairs of contact points between the gripper and the target object. 

%For robotic grasping, we identify the \textit{contact points} between the gripper and the target object. We register the contacts points on the canonical mesh of our target object class, and warp them to conform to novel object instances. For example, our algorithm alters the pose of the gripper in order to grasp the rim of a mug based on how tall the mug is and what its orientation is. For object placement and trajectory cloning, we identify pairs \textit{nearby points} between the pair of objects (e.g. a mug and a tree). We anchor the points on the canonical counterparts of the pair of objects, and solve for the relative pose of a new pair of objects using a pairwise best fit algorithm.

%We broaden the shape warping paradigm to include the \textit{warping of actions}. Instead of matching descriptors attached to 3D points (NDFs), or computing cross-attention between point clouds (TAX-Pose), we warp the pick and place poses, shown in a single demonstration of a task, to conform to novel object shapes. Specifically, for pick actions, we select a point on the canonical object closest to the contact between the gripper and the object. The position of the gripper translates as the canonical object grows and shrinks during warping, and the pose of the gripper rotates with the object. For place action cloning \evdp{action place cloning has not been introduced yet}, we identify pairs of \textit{contact points} between the pair of objects involved in the action, e.g. a mug and a mug-tree, or a mug and the ground. The contact points transform as their parent object is warped, and a new target pose is computed to connect the pairs of warped contact points as closely as possible. Our method is $\mathrm{SE}(3)$ equivariant, as the demonstrated pick and place actions are registered on the canonical object instance in a canonical reference frame. When we warp pick actions and place contact points to novel object instances, we again do so irrespective of to the pose of the objects. Object pose only affects our ability to perceive them \evdp{what does `them' refer to?}, as we work with partial point clouds.
%\lsw{Readers unlikely familiar with equivariance, go gentle. There are some important points here that are worth more sentences. (Also why is this paragraph repeated?)}

%We contribute a new inference method for SST-W \cite{thompson21shapebased} \evdp{add short description of new inference method} and a novel pick-and-place warping method that leverages canonicalization.
%\lsw{With all due credit and respect to Rory, this might be underselling because it sounds like you created a small extension specific to a niche method. A bolder move is to just claim a new method, and then later qualify (as you have in Section IV-B) that you build on SST-W.}
%Unlike prior work, we require only five to ten examples of an object class to learn our object representation. We perform real-world experiments on X $\mathrm{SE}(3)$ pick-and-place tasks using a full perception and motion planning pipeline described in section Y. We find Z. We also isolate the problem of relative pose prediction (as done in prior work \cite{pan22taxpose,simeonov22neural}), and run a comparison in simulation.

% Any robot deployed in the world can expect to encounter things it has not seen before, and an effective robot should be able to manipulate them. For instance, in an unfamiliar home, a robot should be able to hang an unfamiliar mug on an unfamiliar hook. \tk{Maybe say something about how representing the 3D world really matters for robotics, i.e. a bit more of a general statement before you go into the specific example of object shape generalization} To do so, a robot has to generalize over object shapes and manipulate objects in any 3D position and orientation. It should also generalize to a re-arrangement of objects in a scene, a property we call $\mathrm{SE}(3)$-equivariance. \evdp{re-arrangement of objects in a scene seems more related to decomposition/permutation/factorization generalization, not SE3 equivariance}\tk{+1, I think a better example could be that it should e.g. be able to generalize to a mug placed sideways on the floor, even though it only ever saw demonstrations for mugs placed upright on a table or counter}

%Classical robotics approaches use primitive shapes \citep{miller03automatic,tenorth13decomposing} or fully-specified CAD models \citep{klank09realtime,beetz11robotic} to model objects, and describe the world and tasks using symbolic models with limited scope \citep{kaelbling11hierarchical,bartels13constraintbased} \evdp{vague, how and why is the scope limited?}. Hence, their generality is limited. Recently, deep learning methods have begun to make inroads in this problem by employing large generalizable networks trained on similarly large datasets []. While deep learning systems learn impressive policies, they are often limited to top-down manipulation \citep{levine18learning,zhu22sample} \tk{Maybe have a short sentence explaining what top-down manipulation means?} and can require thousands of interactions with specific object instances \citep{yu19metaworld,wang22mathrmso}.%, and sometimes involve learning with only low-level percepts, such as object positions and velocities, unable to generalize to novel objects \citep{haarnoja18Soft,li20Practical}.

%More recently, Neural Descriptor Fields (NDFs) \cite{simeonov22neural,simeonov22se} and TAX-Pose \cite{pan22taxpose} achieved sample-efficient learning of $\mathrm{SE}(3)$-equivariant object re-arrangement policies. These methods assume the ability to segment objects in a scene, allowing them to focus on representing the shapes of individual objects. Shape representations are learned with deep point cloud \cite{qi17pointnet,qi17pointneta} or mesh \cite{mescheder19occupancy,park19deepsdf} encoders using on the order of hundreds of example object meshes per object class \cite{chang15shapenet}. The key to sample-efficiency of NDFs and TAX-Pose is the ability to assign a descriptive feature vector to 3D points around the objects of interest. NDFs directly match target poses of objects across different instances using gradient descent, where TAX-Pose computes cross-attention between encodings of individual points in a point cloud. The matching of 3D features is learned from about 10 demonstrations of a particular task.

%These are used to either directly match target poses of objects across different instances \cite{simeonov22Neural,simeonov22Neurala}, or are further processed by cross-attention layers \cite{vaswani17Attention}. 

% 1. Importance of open-world pick-and-place; SE(3); deep learning might make it possible, but i t's limited to closed domains,
% many demonstrations, lack of generalization.
% E.g. the original GNN factored block stacking paper doesn't even see the objects it manipulates.

% Among many desired capabilities, a robot should be able to generalize to novel instances of objects \evdp{can we add a concrete example?}, and should be able to synthesize pick-and-place actions in $\mathrm{SE}(3)$, enabling it to arrange objects in any position and orientation. Many past robotics works have tackled these challenges. 

%Yet, they were often limited to modeling objects using primitive shapes \citep{miller03Automatic,tenorth13Decomposing}, or fully-specified CAD models \citep{klank09Realtime,beetz11Robotic}, and describing the world and tasks using symbolic models with limited scope \citep{kaelbling11Hierarchical,bartels13Constraintbased}. % OB: Four of these papers are from the same German group, should diversify.

%% OB: Perhaps we don't need this paragraph.
%Recently, deep learning was posed to overcome these challenges with large generalizable neural networks trained on large datasets \citep{schmidhuber15Deep} \evdp{instead of a generic citation, you should add a bunch of citations of deep learning for robotics (/grasping)} \lsw{or a particularly recognizable survey paper}.
%While deep learning systems learn impressive policies,  they are often limited to top-down manipulation \citep{levine18Learning,zhu22Sample}, can require (at least) tens of thousands of interactions with specific object instances \citep{yu19MetaWorld,wang22MathrmSO}, and sometimes involve learning with only low-level percepts, such as object positions and velocities, unable to generalize to novel objects \citep{haarnoja18Soft,li20Practical}. % OB: SAC might not be the best citation here.

% 2. Object factorization and shape understanding can greatly improve sample efficiency.
% However, these approaches require large libraries of objects.
% Moreover, the way we extract feature vectors from all NN activations (do all models do this?) is tricky.

%\lsw{The following two paragraphs feel a bit like a related work section; on a related note, the intro is quite long.}
%Factoring scenes into individual object states \citep{janner19Reasoning,kipf20Contrastive} and representing the appearance of these objects using geometric deep learning methods \citep{mescheder19Occupancy,park19DeepSDF,mildenhall20NeRF,sajjadiObject} can greatly increase sample-efficiency and generality of robot learning systems. Recent works demonstrate class-level $\mathrm{SE}(3)$ manipulation learned from a handful of demonstrations \citep{simeonov22Neural,pan22TAXPose,simeonov22Neurala,wen22You}. The key to sample-efficiency of these methods is the ability to assign a descriptive feature vector to 3D points around the object of interest. These are used to either directly match target poses of objects across different instances \cite{simeonov22Neural,simeonov22Neurala}, or are further processed by cross-attention layers \cite{vaswani17Attention}. 

%\lsw{Write out the full name the first time you introduce something.}
%NDFs \citep{simeonov22Neural} assign a descriptor to an arbitrary point in 3D using a pre-trained Occupancy Network \cite{mescheder19Occupancy}, which contains information about the local geometry around the point. TAX-Pose \citep{pan22TAXPose} pre-train \ob{double-check} a point cloud encoder using contrastive learning \cite{oord19Representation}, so that the points corresponding to, for example, the handle of a mug have similar feature vectors across object instances. As a consequence, these methods require access to moderately large libraries of realistic CAD models \cite{chang15ShapeNet}.
%\lsw{Quantify what is ``moderately large''?}
%Further, spatial feature learning is still an open question; e.g., the concatenation of all intermediate neural activations of an Occupancy Network, as done in NDFs, is not a compact 3D point descriptor. Contrastive learning can be sensitive to the choice of negative samples \cite{biza21Impact}. % Could not resist the self cite :> 

% OB: How does "You Only Demonstrate Once" fit into this?

%Yet more recently, ... object-factorization [C-SWM, O2P2] ... object shape understanding [occupancy networks, deep sdfs, nerfs, osrts] ... highly sample-efficient SE(3) pick-and-place

% 3. In another line of work, shape warping has been used to clone grasps.
% These methods are limited to full point clouds and usually clone grasps only (not all I guess, there's the fabric manipulation paper).
% Recently, Skye proposed to learn a low-dimensional latent space that warps a canonical object, and fit it to new shapes using gradient descent.
% 4. We propose to use canonicalization for both shape warping and pick-and-place cloning. Given a single demonstration, we register the pick pose and the contact points between the held object and the target during placement. As the shape of the canonical object warps to conform to a new instance, the pick and place poses warp correspondingly.
% Additionally, we also warp the faces of the object, giving us a collision mesh for planning.
% Our method is $\mathrm{SE}(3)$ equivariant, as pick and place poses are predicted in a reference frame invariant to the current pose of the objects in the scene.

% We extend the shape warping paradigm to include the \textit{warping of actions}. Instead of matching descriptors attached to 3D points \citep{simeonov22Neural}, or computing cross-attention between point clouds \citep{pan22TAXPose}, we warp the pick and place poses, shown in a single demonstration of a task, to conform to novel object shapes. Specifically, for pick actions, we select a point on the canonical object closest to the contact between the gripper and the object. The position of the gripper translates as the canonical objects grows and shrinks during warping, and the pose of the gripper rotates with the object. For place action cloning, we identify pairs of \textit{contact points} between the pair of objects involved in the action, e.g. a mug and a mug-tree, or a mug and the ground. The contact points transform as their parent object is warped, and a new target pose is computed to connect the pairs of warped contact points as closely as possible. Our method is $\mathrm{SE}(3)$ equivariant, as the demonstrated pick and place actions are registered on the canonical object instance in a canonical reference frame. When we warp pick actions and place contact points to novel object instances, we again do so irresponsive \evdp{?} to the pose of the objects. Object pose only affects our ability to perceive them \evdp{what does `them' refer to?}, as we work with partial point clouds.

%we can both \textit{detect a class of object} and \textit{understand the intra-class shape variation} from few examples (around 10). 
% 1. Many prior SE(3) behavior cloning works do not use any pre-trained information about objects. Then, they need to see many demonstrations to understand both the task and the objects that are involved in it.
% 2. Prior object pre-training methods use large libraries of hand-crafted 3D meshes of objects. This approach would not work for specialized objects, e.g. in a manufacturing setting. \evdp{Why not? Could we craft 3D meshes of specialized objects?}
% 3. We propose a simple method based on warping pick locations and warping and matching contact points during object placement.
% \evdp{can you make a bullet list of contributions?}